{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一，Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.txt is already downloaded.\n",
      "File data/validation.txt is already downloaded.\n",
      "File data/test.txt is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week2_resources\n",
    "\n",
    "download_week2_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "\n",
    "            ######################################\n",
    "            ######### YOUR CODE HERE #############\n",
    "            ######################################\n",
    "            if(token.startswith('@')):\n",
    "                token = '<USR>'\n",
    "            elif(token.startswith('http://') or token.startswith('https://')):\n",
    "                token = '<URL>'\n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "\n",
      "Apple\tB-product\n",
      "MacBook\tI-product\n",
      "Pro\tI-product\n",
      "A1278\tI-product\n",
      "13.3\tI-product\n",
      "\"\tI-product\n",
      "Laptop\tI-product\n",
      "-\tI-product\n",
      "MD101LL/A\tI-product\n",
      "(\tO\n",
      "June\tO\n",
      ",\tO\n",
      "2012\tO\n",
      ")\tO\n",
      "-\tO\n",
      "Full\tO\n",
      "read\tO\n",
      "by\tO\n",
      "eBay\tB-company\n",
      "<URL>\tO\n",
      "<URL>\tO\n",
      "\n",
      "Happy\tO\n",
      "Birthday\tO\n",
      "<USR>\tO\n",
      "!\tO\n",
      "May\tO\n",
      "Allah\tB-person\n",
      "s.w.t\tO\n",
      "bless\tO\n",
      "you\tO\n",
      "with\tO\n",
      "goodness\tO\n",
      "and\tO\n",
      "happiness\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  collections  import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    # Create mappings from tokens to indices and vice versa\n",
    "    # Add special tokens to dictionaries\n",
    "    # The first special token must have index 0\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    idx = 0\n",
    "    for token in special_tokens:\n",
    "        idx2tok.append(token)\n",
    "        tok2idx[token] = idx\n",
    "        idx += 1\n",
    "\n",
    "    for token_list in tokens_or_tags:\n",
    "        for token in token_list:\n",
    "            if token not in tok2idx:\n",
    "                idx2tok.append(token)\n",
    "                tok2idx[token] = idx\n",
    "                idx += 1\n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二，Build RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ymh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "    # Placeholders for input and ground truth output.\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
    "  \n",
    "    # Placeholder for lengths of the sequences.\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths')\n",
    "    \n",
    "    # Placeholder for a dropout keep probability. If we don't feed\n",
    "    # a value for this placeholder, it will be equal to 1.0.\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder_with_default(1e4, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "    \n",
    "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.Variable(initial_embedding_matrix, name='embeddings_matrix', dtype=tf.float32)\n",
    "    \n",
    "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
    "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn, forget_bias=3.0),\n",
    "        input_keep_prob=self.dropout_ph,\n",
    "        output_keep_prob=self.dropout_ph,\n",
    "        state_keep_prob=self.dropout_ph\n",
    "    )\n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn, forget_bias=3.0),\n",
    "        input_keep_prob=self.dropout_ph,\n",
    "        output_keep_prob=self.dropout_ph,\n",
    "        state_keep_prob=self.dropout_ph\n",
    "    )\n",
    "\n",
    "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
    "    \n",
    "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "    (rnn_output_fw, rnn_output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw= forward_cell, cell_bw= backward_cell,\n",
    "        dtype=tf.float32,\n",
    "        inputs=embeddings,\n",
    "        sequence_length=self.lengths\n",
    "    )\n",
    "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "\n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, sequence_len, n_tags].   \n",
    "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
    "    \n",
    "    # Create softmax (tf.nn.softmax) function\n",
    "    softmax_output = tf.nn.softmax(self.logits)\n",
    "    \n",
    "    # Use argmax (tf.argmax) to get the most probable tags\n",
    "    # Don't forget to set axis=-1\n",
    "    # otherwise argmax will be calculated in a wrong way\n",
    "    self.predictions = tf.argmax(softmax_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, n_tags, PAD_index):\n",
    "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
    "    \n",
    "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_tags_one_hot, logits=self.logits)\n",
    "    \n",
    "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
    "    mask = tf.cast(tf.not_equal(loss_tensor, PAD_index), tf.float32)\n",
    "    self.loss =  tf.reduce_mean(tf.reduce_sum(tf.multiply(loss_tensor, mask), axis=-1) / tf.reduce_sum(mask, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "    \n",
    "    # Create an optimizer (tf.train.AdamOptimizer)\n",
    "    self.optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
    "    # Pay attention that you need to apply this operation only for gradients \n",
    "    # because self.grads_and_vars contains also variables.\n",
    "    # list comprehension might be useful in this case.\n",
    "    clip_norm = tf.cast(1.0, tf.float32)\n",
    "    self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in self.grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(n_tags, PAD_index)\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三，Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    predictions = session.run(self.predictions, feed_dict={self.input_batch:x_batch, self.lengths:lengths})\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四，Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import precision_recall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, session, token_idxs_batch, lengths):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
    "    \n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            tags.append(idx2tag[tag_idx])\n",
    "            tokens.append(idx2token[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "    \n",
    "def eval_conll(model, session, tokens, tags, short_report=True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五，Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-aa8c7c55ca6b>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(20505, 21, 200, 200, token2idx['<PAD>'])\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 4\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 1.414\n",
    "dropout_keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 64872 phrases; correct: 136.\n",
      "\n",
      "precision:  0.21%; recall:  3.03%; F1:  0.39\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 0.5670220004536176),\n",
       "                           ('recall', 3.8880248833592534),\n",
       "                           ('f1', 0.9897070467141725),\n",
       "                           ('n_predicted_entities', 4409),\n",
       "                           ('n_true_entities', 643)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 0.10174665083940986),\n",
       "                           ('recall', 1.910828025477707),\n",
       "                           ('f1', 0.1932056029624859),\n",
       "                           ('n_predicted_entities', 5897),\n",
       "                           ('n_true_entities', 314)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 0.5826112936958471),\n",
       "                           ('recall', 3.91566265060241),\n",
       "                           ('f1', 1.0143042912873863),\n",
       "                           ('n_predicted_entities', 6694),\n",
       "                           ('n_true_entities', 996)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0.008899172376968943),\n",
       "                           ('recall', 1.4705882352941175),\n",
       "                           ('f1', 0.017691287041132243),\n",
       "                           ('n_predicted_entities', 11237),\n",
       "                           ('n_true_entities', 68)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 0.11936735302894659),\n",
       "                           ('recall', 3.4482758620689653),\n",
       "                           ('f1', 0.23074704355350448),\n",
       "                           ('n_predicted_entities', 6702),\n",
       "                           ('n_true_entities', 232)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 0.18453589223103894),\n",
       "                           ('recall', 1.321003963011889),\n",
       "                           ('f1', 0.3238341968911917),\n",
       "                           ('n_predicted_entities', 5419),\n",
       "                           ('n_true_entities', 757)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 0.4322200392927309),\n",
       "                           ('recall', 2.4830699774266365),\n",
       "                           ('f1', 0.7362784471218207),\n",
       "                           ('n_predicted_entities', 5090),\n",
       "                           ('n_true_entities', 886)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 0.1313370107696349),\n",
       "                           ('recall', 1.5723270440251573),\n",
       "                           ('f1', 0.24242424242424246),\n",
       "                           ('n_predicted_entities', 3807),\n",
       "                           ('n_true_entities', 318)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 0.18980734554427256),\n",
       "                           ('recall', 9.216589861751153),\n",
       "                           ('f1', 0.3719546215361726),\n",
       "                           ('n_predicted_entities', 10537),\n",
       "                           ('n_true_entities', 217)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 5080),\n",
       "                           ('n_true_entities', 58)]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 8081 phrases; correct: 18.\n",
      "\n",
      "precision:  0.22%; recall:  3.35%; F1:  0.42\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 0.3215434083601286),\n",
       "                           ('recall', 1.9230769230769231),\n",
       "                           ('f1', 0.5509641873278237),\n",
       "                           ('n_predicted_entities', 622),\n",
       "                           ('n_true_entities', 104)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 751),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 0.4662004662004662),\n",
       "                           ('recall', 3.5398230088495577),\n",
       "                           ('f1', 0.8238928939237898),\n",
       "                           ('n_predicted_entities', 858),\n",
       "                           ('n_true_entities', 113)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 1353),\n",
       "                           ('n_true_entities', 7)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 0.11587485515643105),\n",
       "                           ('recall', 3.571428571428571),\n",
       "                           ('f1', 0.22446689113355775),\n",
       "                           ('n_predicted_entities', 863),\n",
       "                           ('n_true_entities', 28)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 0.16393442622950818),\n",
       "                           ('recall', 1.2345679012345678),\n",
       "                           ('f1', 0.2894356005788712),\n",
       "                           ('n_predicted_entities', 610),\n",
       "                           ('n_true_entities', 81)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 1.25),\n",
       "                           ('recall', 7.142857142857142),\n",
       "                           ('f1', 2.127659574468085),\n",
       "                           ('n_predicted_entities', 640),\n",
       "                           ('n_true_entities', 112)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 0.1851851851851852),\n",
       "                           ('recall', 2.941176470588235),\n",
       "                           ('f1', 0.34843205574912894),\n",
       "                           ('n_predicted_entities', 540),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 0.0831255195344971),\n",
       "                           ('recall', 5.0),\n",
       "                           ('f1', 0.1635322976287817),\n",
       "                           ('n_predicted_entities', 1203),\n",
       "                           ('n_true_entities', 20)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 641),\n",
       "                           ('n_true_entities', 4)]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 2 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 2665 phrases; correct: 513.\n",
      "\n",
      "precision:  19.25%; recall:  11.43%; F1:  14.34\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 37.13235294117647),\n",
       "                           ('recall', 15.707620528771384),\n",
       "                           ('f1', 22.07650273224044),\n",
       "                           ('n_predicted_entities', 272),\n",
       "                           ('n_true_entities', 643)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 401),\n",
       "                           ('n_true_entities', 314)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 24.16225749559083),\n",
       "                           ('recall', 41.265060240963855),\n",
       "                           ('f1', 30.478309232480534),\n",
       "                           ('n_predicted_entities', 1701),\n",
       "                           ('n_true_entities', 996)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 68)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 2),\n",
       "                           ('n_true_entities', 232)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 1.639344262295082),\n",
       "                           ('recall', 0.13210039630118892),\n",
       "                           ('f1', 0.24449877750611249),\n",
       "                           ('n_predicted_entities', 61),\n",
       "                           ('n_true_entities', 757)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 225),\n",
       "                           ('n_true_entities', 886)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 3),\n",
       "                           ('n_true_entities', 318)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 217)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 58)]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 228 phrases; correct: 39.\n",
      "\n",
      "precision:  17.11%; recall:  7.26%; F1:  10.20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 30.76923076923077),\n",
       "                           ('recall', 11.538461538461538),\n",
       "                           ('f1', 16.783216783216783),\n",
       "                           ('n_predicted_entities', 39),\n",
       "                           ('n_true_entities', 104)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 35),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 19.28571428571429),\n",
       "                           ('recall', 23.893805309734514),\n",
       "                           ('f1', 21.343873517786566),\n",
       "                           ('n_predicted_entities', 140),\n",
       "                           ('n_true_entities', 113)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 7)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 28)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 2),\n",
       "                           ('n_true_entities', 81)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 12),\n",
       "                           ('n_true_entities', 112)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 20)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 4)]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 3 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4409 phrases; correct: 1719.\n",
      "\n",
      "precision:  38.99%; recall:  38.29%; F1:  38.64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 63.879003558718864),\n",
       "                           ('recall', 55.83203732503888),\n",
       "                           ('f1', 59.5850622406639),\n",
       "                           ('n_predicted_entities', 562),\n",
       "                           ('n_true_entities', 643)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 21.031207598371775),\n",
       "                           ('recall', 49.36305732484077),\n",
       "                           ('f1', 29.495718363463364),\n",
       "                           ('n_predicted_entities', 737),\n",
       "                           ('n_true_entities', 314)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 45.62807881773399),\n",
       "                           ('recall', 74.3975903614458),\n",
       "                           ('f1', 56.56488549618321),\n",
       "                           ('n_predicted_entities', 1624),\n",
       "                           ('n_true_entities', 996)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 68)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 232)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 39.40886699507389),\n",
       "                           ('recall', 10.568031704095112),\n",
       "                           ('f1', 16.666666666666668),\n",
       "                           ('n_predicted_entities', 203),\n",
       "                           ('n_true_entities', 757)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 34.72882968601332),\n",
       "                           ('recall', 41.19638826185101),\n",
       "                           ('f1', 37.687145069695404),\n",
       "                           ('n_predicted_entities', 1051),\n",
       "                           ('n_true_entities', 886)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 8.189655172413794),\n",
       "                           ('recall', 5.9748427672955975),\n",
       "                           ('f1', 6.909090909090909),\n",
       "                           ('n_predicted_entities', 232),\n",
       "                           ('n_true_entities', 318)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 217)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 58)]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 332 phrases; correct: 123.\n",
      "\n",
      "precision:  37.05%; recall:  22.91%; F1:  28.31\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 54.054054054054056),\n",
       "                           ('recall', 38.46153846153847),\n",
       "                           ('f1', 44.94382022471911),\n",
       "                           ('n_predicted_entities', 74),\n",
       "                           ('n_true_entities', 104)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 18.51851851851852),\n",
       "                           ('recall', 29.411764705882355),\n",
       "                           ('f1', 22.72727272727273),\n",
       "                           ('n_predicted_entities', 54),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 44.642857142857146),\n",
       "                           ('recall', 44.24778761061947),\n",
       "                           ('f1', 44.44444444444445),\n",
       "                           ('n_predicted_entities', 112),\n",
       "                           ('n_true_entities', 113)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 7)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 28)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 68.75),\n",
       "                           ('recall', 13.580246913580247),\n",
       "                           ('f1', 22.680412371134018),\n",
       "                           ('n_predicted_entities', 16),\n",
       "                           ('n_true_entities', 81)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 22.22222222222222),\n",
       "                           ('recall', 10.714285714285714),\n",
       "                           ('f1', 14.457831325301203),\n",
       "                           ('n_predicted_entities', 54),\n",
       "                           ('n_true_entities', 112)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 22),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 20)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 4)]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 4 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4744 phrases; correct: 2599.\n",
      "\n",
      "precision:  54.78%; recall:  57.90%; F1:  56.30\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 71.34052388289676),\n",
       "                           ('recall', 72.00622083981337),\n",
       "                           ('f1', 71.671826625387),\n",
       "                           ('n_predicted_entities', 649),\n",
       "                           ('n_true_entities', 643)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 58.36734693877551),\n",
       "                           ('recall', 45.54140127388535),\n",
       "                           ('f1', 51.16279069767442),\n",
       "                           ('n_predicted_entities', 245),\n",
       "                           ('n_true_entities', 314)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 74.93087557603687),\n",
       "                           ('recall', 81.62650602409639),\n",
       "                           ('f1', 78.13551177318597),\n",
       "                           ('n_predicted_entities', 1085),\n",
       "                           ('n_true_entities', 996)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 68)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 25.0),\n",
       "                           ('recall', 0.43103448275862066),\n",
       "                           ('f1', 0.847457627118644),\n",
       "                           ('n_predicted_entities', 4),\n",
       "                           ('n_true_entities', 232)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 38.62781954887218),\n",
       "                           ('recall', 54.29326287978864),\n",
       "                           ('f1', 45.14003294892916),\n",
       "                           ('n_predicted_entities', 1064),\n",
       "                           ('n_true_entities', 757)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 47.43589743589743),\n",
       "                           ('recall', 83.52144469525959),\n",
       "                           ('f1', 60.50695012264922),\n",
       "                           ('n_predicted_entities', 1560),\n",
       "                           ('n_true_entities', 886)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 20.437956204379564),\n",
       "                           ('recall', 8.80503144654088),\n",
       "                           ('f1', 12.307692307692308),\n",
       "                           ('n_predicted_entities', 137),\n",
       "                           ('n_true_entities', 318)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 217)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 58)]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 371 phrases; correct: 153.\n",
      "\n",
      "precision:  41.24%; recall:  28.49%; F1:  33.70\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 59.72222222222222),\n",
       "                           ('recall', 41.34615384615385),\n",
       "                           ('f1', 48.86363636363636),\n",
       "                           ('n_predicted_entities', 72),\n",
       "                           ('n_true_entities', 104)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 40.0),\n",
       "                           ('recall', 17.647058823529413),\n",
       "                           ('f1', 24.48979591836735),\n",
       "                           ('n_predicted_entities', 15),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 65.4320987654321),\n",
       "                           ('recall', 46.902654867256636),\n",
       "                           ('f1', 54.63917525773196),\n",
       "                           ('n_predicted_entities', 81),\n",
       "                           ('n_true_entities', 113)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 7)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 28)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 22.22222222222222),\n",
       "                           ('recall', 27.160493827160494),\n",
       "                           ('f1', 24.444444444444443),\n",
       "                           ('n_predicted_entities', 99),\n",
       "                           ('n_true_entities', 81)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 29.896907216494846),\n",
       "                           ('recall', 25.892857142857146),\n",
       "                           ('f1', 27.751196172248804),\n",
       "                           ('n_predicted_entities', 97),\n",
       "                           ('n_true_entities', 112)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 7),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 20)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 0),\n",
       "                           ('n_true_entities', 4)]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train set quality: --------------------\n",
      "processed 105778 tokens with 4489 phrases; found: 4833 phrases; correct: 3157.\n",
      "\n",
      "precision:  65.32%; recall:  70.33%; F1:  67.73\n",
      "\n",
      "\t     company: precision:   77.79%; recall:   83.36%; F1:   80.48; predicted:   689\n",
      "\n",
      "\t    facility: precision:   66.35%; recall:   66.56%; F1:   66.45; predicted:   315\n",
      "\n",
      "\t     geo-loc: precision:   77.85%; recall:   91.06%; F1:   83.94; predicted:  1165\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t musicartist: precision:   27.08%; recall:    5.60%; F1:    9.29; predicted:    48\n",
      "\n",
      "\t       other: precision:   56.02%; recall:   68.82%; F1:   61.77; predicted:   930\n",
      "\n",
      "\t      person: precision:   62.96%; recall:   92.66%; F1:   74.98; predicted:  1304\n",
      "\n",
      "\t     product: precision:   35.29%; recall:   39.62%; F1:   37.33; predicted:   357\n",
      "\n",
      "\t  sportsteam: precision:   96.00%; recall:   11.06%; F1:   19.83; predicted:    25\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "-------------------- Validation set quality: --------------------\n",
      "processed 12836 tokens with 537 phrases; found: 411 phrases; correct: 175.\n",
      "\n",
      "precision:  42.58%; recall:  32.59%; F1:  36.92\n",
      "\n",
      "\t     company: precision:   62.03%; recall:   47.12%; F1:   53.55; predicted:    79\n",
      "\n",
      "\t    facility: precision:   38.46%; recall:   29.41%; F1:   33.33; predicted:    26\n",
      "\n",
      "\t     geo-loc: precision:   63.83%; recall:   53.10%; F1:   57.97; predicted:    94\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t musicartist: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     4\n",
      "\n",
      "\t       other: precision:   26.00%; recall:   32.10%; F1:   28.73; predicted:   100\n",
      "\n",
      "\t      person: precision:   35.29%; recall:   26.79%; F1:   30.46; predicted:    85\n",
      "\n",
      "\t     product: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:    23\n",
      "\n",
      "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "-------------------- Test set quality: --------------------\n",
      "processed 13258 tokens with 604 phrases; found: 493 phrases; correct: 226.\n",
      "\n",
      "precision:  45.84%; recall:  37.42%; F1:  41.20\n",
      "\n",
      "\t     company: precision:   54.24%; recall:   38.10%; F1:   44.76; predicted:    59\n",
      "\n",
      "\t    facility: precision:   40.54%; recall:   31.91%; F1:   35.71; predicted:    37\n",
      "\n",
      "\t     geo-loc: precision:   70.08%; recall:   53.94%; F1:   60.96; predicted:   127\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t musicartist: precision:   28.57%; recall:    7.41%; F1:   11.76; predicted:     7\n",
      "\n",
      "\t       other: precision:   29.73%; recall:   32.04%; F1:   30.84; predicted:   111\n",
      "\n",
      "\t      person: precision:   45.69%; recall:   50.96%; F1:   48.18; predicted:   116\n",
      "\n",
      "\t     product: precision:    5.56%; recall:    7.14%; F1:    6.25; predicted:    36\n",
      "\n",
      "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
